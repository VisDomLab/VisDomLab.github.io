[ 
  {
    "title": "CosFairNet: A Parameter-Space based Approach for Bias Free Learning",
    "image": "/images/publications/CosFairNet.png",
    "authors": "Rajeev Ranjan Dwivedi, Priyadarshni Kumari, Vinod K Kurmi",
    "conference": "British Machine Vision Conference (BMVC), (2024)",
    "links":{
      "page": "https://visdomlab.github.io/CosFairNet/",
      "pdf":"https://visdomlab.github.io/CosFairNet/static/pdfs/Fairnesss_BMVC_2024.pdf"
    },
    "details":{
      "abstract":"Deep neural networks trained on biased data often inadvertently learn unintended inference rules, particularly when labels are strongly correlated with biased features. Existing bias mitigation methods typically involve either a) predefining bias types and enforcing them as prior knowledge or b) reweighting training samples to emphasize bias-conflicting samples over bias-aligned samples. However, both strategies address bias indirectly in the feature or sample space, with no control over learned weights, making it difficult to control the bias propagation across different layers. Based on this observation, we introduce a novel approach to address bias directly in the model's parameter space, preventing its propagation across layers. Our method involves training two models: a bias model for biased features and a debias model for unbiased details, guided by the bias model. We enforce dissimilarity in the debias model's later layers and similarity in its initial layers with the bias model, ensuring it learns unbiased low-level features without adopting biased high-level abstractions. By incorporating this explicit constraint during training, our approach shows enhanced classification accuracy and debiasing effectiveness across various synthetic and real-world datasets of different sizes. Moreover, the proposed method demonstrates robustness across different bias types and percentages of biased samples in the training data.",
      "bibtex":"@InProceedings{rajeevbmvc24_abs,\nauthor    = {Dwivedi, Rajeev R \nand Kumari, Priyadarshini \nand Kurmi, Vinod},\ntitle     = {CosFairNet:A Parameter-Space based \nApproach for Bias Free Learning},\nbooktitle = {Proceedings of the British Machine \nVision Conference (BMVC)},\nyear      = {2024},\n}"
    }    
  },
  {
    "title": "Quantifying Uncertainty in Neural Networks through Residuals",
    "image": "/images/publications/Quantifying.png",
    "authors": "Udhbav Dalavi, Rini Smita Thakur, Rajeev Ranjan Dwivedi, Vinod K Kurmi",
    "conference": "33rd ACM International Conference on Information and Knowledge Management (CIKM), (2024)",
    "links":{
      "page" :"https://visdomlab.github.io/HetGP/",
      "pdf":"https://visdomlab.github.io/HetGP/Quantifying_uncertainty_in_Neural_Networks_through_residuals.pdf"
    },
    "details":{
      "abstract":"Regression models are of fundamental importance in explicitly explaining the response variable in terms of covariates. Heteroscedasticity is common in most real-world scenarios and is hard to model due to its randomness. The Gaussian process generally captures epistemic (model) uncertainty but fails to capture heteroscedastic aleatoric uncertainty. The framework of Heteroscedastic Gaussian Process (HetGP) inherently captures both epistemic and aleatoric by placing independent Gaussian processes on both mean function and error term. In this work, we propose the posthoc HetGP on the residuals of the trained neural network to obtain both epistemic and aleatoric uncertainty. The advantage of posthoc HetGP on residuals is that it can be extended to any type of model, since the model is assumed to be black-box that gives point predictions. We demonstrate our approach through simulation studies on UCI regression datasets.",
      "bibtex":"@inproceedings{udbhav,\n Author = {Udbhav Mallanna,Dalavai \nand Dwivedi, Rajeev R \nand Thakur, Rini S \nand Kurmi, Vinod }, \nTitle = {Quantifying Uncertainty \nin Neural Networks through Residuals},\nBooktitle = {CIKM},\nYear   = {2024}\n}"
    }    
  },
  {
    "title": "Towards Robust Few-shot Class Incremental Learning in Audio Classification using Contrastive Representation",
    "image": "/images/publications/audio_classification.png",
    "authors": "Riyansha Singh, Parinita Nema, Vinod K Kurmi",
    "conference": "Interspeech 2024",
    "links":{
      "page" :"https://visdomlab.github.io/FsACLearning/",
      "pdf":"https://arxiv.org/abs/2407.19265"
    },
    "details":{
      "abstract":"In machine learning applications, gradual data ingress is common, especially in audio processing where incremental learning is vital for real-time analytics. Few-shot class-incremental learning addresses challenges arising from limited incoming data. Existing methods often integrate additional trainable components or rely on a fixed embedding extractor post-training on base sessions to mitigate concerns related to catastrophic forgetting and the dangers of model overfitting. However, using cross-entropy loss alone during base session training is suboptimal for audio data. To address this, we propose incorporating supervised contrastive learning to refine the representation space, enhancing discriminative power and leading to better generalization since it facilitates seamless integration of incremental classes, upon arrival. Experimental results on NSynth and LibriSpeech datasets with 100 classes, as well as ESC dataset with 50 and 10 classes, demonstrate state-of-the-art performance.",
      "bibtex":"@article{singh2024towards,\ntitle={Towards Robust Few-shot Class \nIncremental Learning in Audio Classification \nusing Contrastive Representation},\nauthor={Singh, Riyansha \nand Nema, Parinita \nand Kurmi, Vinod K},\njournal={arXiv preprint arXiv:2407.19265},\nyear={2024}\n}"
    }    
  },
  {
    "title": "Distilling Knowledge for Occlusion Robust Monocular 3D Face Reconstruction",
    "image": "/images/publications/ivc_3.jpg",
    "authors": "H. Tiwari, Vinod K Kurmi, Venkatesh K Subramanian, Yong-Sheng Chen",
    "conference": "Image and Vision Computing,(IMAVIS), 2023",
    "links":{
      "pdf":"https://sciencedirect.com/science/article/abs/pii/S0262885623001373"
    },
    "details":{
      "abstract":"Recently, there have been significant advancements in the 3D face reconstruction field, largely driven by monocular image-based deep learning methods. However, these methods still face challenges in reliable deployments due to their sensitivity to facial occlusions and inability to maintain identity consistency across different occlusions within the same facial image. To address these issues, we propose two frameworks: Distillation Assisted Mono Image Occlusion Robustification (DAMIOR) and Duplicate Images Assisted Multi Occlusions Robustification (DIAMOR). The DAMIOR framework leverages the knowledge from the Occlusion Frail Trainer (OFT) network to enhance robustness against facial occlusions. Our proposed method overcomes the sensitivity to occlusions and improves reconstruction accuracy. To tackle the issue of identity inconsistency, the DIAMOR framework utilizes the estimates from DAMIOR to mitigate inconsistencies in geometry and texture, collectively known as identity, of the reconstructed 3D faces. We evaluate the performance of DAMIOR on two variations of the CelebA test dataset: empirical occlusions and irrational occlusions. Furthermore, we analyze the performance of the proposed DIAMOR framework using the irrational occlusion-based variant of the CelebA test dataset. Our methods outperform state-of-the-art approaches by a significant margin. For example, DAMIOR reduces the 3D vertex-based shape error by 41.1% and the texture error by 21.8% for empirical occlusions. Besides, for facial data with irrational occlusions, DIAMOR achieves a substantial decrease in shape error by 42.5% and texture error by 30.5%. These results demonstrate the effectiveness of our proposed methods.",
      "bibtex":"@inproceedings{hitika_ivc3,\nAuthor = {Tiwari, Hitika\nand Kurmi, Vinod K\nand  Subramanian,\nVenkatesh K and\nChen, Yong Sheng },\nTitle = {Distilling Knowledge\nfor Occlusion Robust Monocular\n3D Face Reconstruction},\nBooktitle = {InterSpeech},\nYear = {2022}\n}"
    }    
  },
  {
    "title": "Generalized Keyword Spotting using ASR embeddings",
    "image": "/images/publications/intersp.jpg",
    "authors": "Kirandevraj R, Vinod K Kurmi, Vinay P Namboodiri, C V Jawahar",
    "conference": "Conference of the International Speech Communication Association (Interspeech) 2022, Incheon Korea",
    "links":{
      "pdf":"https://www.isca-speech.org/archive/pdfs/interspeech_2022/r22_interspeech.pdf"
    },
    "details":{
      "abstract":"Keyword Spotting (KWS) detects a set of pre-defined spoken keywords. Building a KWS system for an arbitrary set re- quires massive training datasets. We propose to use the text transcripts from an Automatic Speech Recognition (ASR) sys- tem alongside triplets for KWS training. The intermediate rep- resentation from the ASR system trained on a speech corpus is used as acoustic word embeddings for keywords. Triplet loss is added to the Connectionist Temporal Classification (CTC) loss in the ASR while training. This method achieves an Average Precision (AP) of 0.843 over 344 words unseen by the model trained on the TIMIT dataset. In contrast, the Multi-View re- current method that learns jointly on the text and acoustic em- beddings achieves only 0.218 for out-of-vocabulary words. This method is also applied to low-resource languages such as Tamil by converting Tamil characters to English using transliteration. This is a very challenging novel task for which we provide a dataset of transcripts for the keywords. Despite our model not generalizing well, we achieve a benchmark AP of 0.321 on over 38 words unseen by the model on the MSWC Tamil keyword set. The model also produces an accuracy of 96.2% for classifi- cation tasks on the Google Speech Commands dataset.",
      "bibtex":"@inproceedings{kiran_inter22,\nAuthor = {R,Kiran.\nand Kurmi, Vinod K\nand Namboodiri, Vinay P\nand Jawhar, CV},\nTitle = {Generalized Keyword\nSpotting using ASR embeddings},\nBooktitle = {InterSpeech},\nYear   = {2022}\n}\n}"
    }    
  }
]
