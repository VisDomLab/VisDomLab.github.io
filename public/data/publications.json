[
  {
    "title": "CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation",
    "image": "/images/publications/CLFSeg.png",
    "authors": "Anshul Kaushal, Kunal Jangid, Vinod K Kurmi",
    "conference": "36th British Machine Vision Conference (BMVC), 2025",
    "links":{
      "page": "https://visdomlab.github.io/CLFSeg/",
      "pdf":""
    },
    "details":{
      "abstract":"Accurate polyp and cardiac segmentation for early detection and treatment is essential for the diagnosis and treatment planning of cancer-like diseases. Traditional convolutional neural network (CNN) based models have represented limited generalizability, robustness, and inability to handle uncertainty, which affects the segmentation performance. To solve these problems, this paper introduces CLFSeg, an encoder-decoder based framework that aggregates the Fuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy logic. This module enhances the segmentation performance by identifying local and global features while minimizing the uncertainty, noise, and ambiguity in boundary regions, ensuring computing efficiency. In order to handle class imbalance problem while focusing on the areas of interest with tiny and boundary regions, binary cross-entropy (BCE) with dice loss is incorporated. Our proposed model exhibits exceptional performance on four publicly available datasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC. Extensive experiments and visual studies show CLFSeg surpasses the existing SOTA performance and focuses on relevant regions of interest in anatomical structures. The proposed CLFSeg improves performance while ensuring computing efficiency, which makes it a potential solution for real-world medical diagnostic scenarios.",
      "bibtex":"@inproceedings{Kaushal_2025_BMVC, \n author    = {Kaushal, Anshul \n and Jangid, Kunal \n and Kurmi, Vinod K}, \n title     = {CLFSeg: A Fuzzy-Logic based \n Solution for Boundary Clarity and Uncertainty \n Reduction in Medical Image Segmentation}, \n booktitle = {Proceedings of the British \n Machine Vision Conference 2025, {BMVC} 2025},  \nyear      = {2025},}\n}"
    }    
  }, 
  {
    "title": "Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment",
    "image": "/images/publications/gcl.png",
    "authors": "Rini Smita Thakur, Rajeev Ranjan Dwivedi, Vinod K Kurmi",
    "conference": "236th British Machine Vision Conference (BMVC), 2025",
    "links":{
      "page": "https://visdomlab.github.io/GCL/",
      "pdf":""
    },
    "details":{
      "abstract":"Accurate segmentation of the optic disc and cup is critical for the early diagnosis and management of ocular diseases such as glaucoma. However, segmentation models trained on one dataset often suffer significant performance degradation when applied to target data acquired under different imaging protocols or conditions. To address this challenge, we propose Grad-CL, a novel source-free domain adaptation framework that leverages a pre-trained source model and unlabeled target data to robustly adapt segmentation performance without requiring access to the original source data. Grad-CL combines a gradient-guided pseudolabel refinement module with a cosine similarity–based contrastive learning strategy. In the first stage, salient class-specific features are extracted via a gradient-based mechanism, enabling more accurate uncertainty quantification and robust prototype estimation for refining noisy pseudolabels. In the second stage, a contrastive loss based on cosine similarity is employed to explicitly enforce inter-class separability between the gradient-informed features of the optic cup and disc. Extensive experiments on challenging cross-domain fundus imaging datasets demonstrate that Grad-CL outperforms state-of-the-art unsupervised and source-free domain adaptation methods, achieving superior segmentation accuracy and improved boundary delineation.",
      "bibtex":"@InProceedings{gradCl_rini_bmvc2025, \n author    = {Thakur, Rini Smita, \n Dwivedi, Rajeev R, \n and Kurmi, Vinod K. },  \n title     = {Grad-CL: Source Free Domain \n Adaptation with Gradient Guided Feature \n Disalignment}, \n booktitle = {Proceedings of the British \n Machine Vision Conference (BMVC)}, \n year      = {2025},}\n}"
    }    
  },  

  {
    "title": "Multi-Attribute Bias Mitigation via Representation Learning",
    "image": "/images/publications/gmbm.png",
    "authors": "Rajeev Ranjan Dwivedi, Ankur Kumar, Vinod K Kurmi",
    "conference": "28th European Conference on Artificial Intelligence (ECAI) 2025",
    "links":{
      "page": "https://visdomlab.github.io/GMBM/",
      "pdf":""
    },
    "details":{
      "abstract":"Real-world images frequently exhibit multiple overlapping biases, including textures, watermarks, gendered makeup, scene-object pairings, etc. These biases collectively impair the performance of modern vision models, undermining both their robustness and fairness. Addressing these biases individually proves inadequate, as mitigating one bias often permits or intensifies others.  We tackle this multi-bias problem with Generalized Multi-Bias Mitigation (GMBM), a lean two-stage framework that needs group labels only while training and minimizes bias at test time. First, Adaptive Bias-Integrated Learning (ABIL) deliberately identifies the influence of known shortcuts by training encoders for each attribute and integrating them with the main backbone, compelling the classifier to explicitly recognize these biases. Then Gradient-Suppression Fine-Tuning prunes those very bias directions from the backbone’s gradients, leaving a single compact network that ignores all the shortcuts it just learned to recognize. Moreover we find that existing bias metrics break under subgroup imbalance and train–test distribution shifts, so we introduce Scaled Bias Amplification (SBA): a test-time measure that disentangles model-induced bias amplification from distributional differences. We validate GMBM on FB-CMNIST, CelebA, and COCO, where we boost worst-group accuracy, halve multi-attribute bias amplification, and set a new low in SBA—even as bias complexity and distribution shifts intensify—making GMBM the first practical, end-to-end multi-bias solution for visual recognition.",
      "bibtex":"@inproceedings{rajeevr_ecai25, \n Author = {Dwivedi, Rajeev R \n  and Kumar, Ankur  \n  and Kurmi, Vinod}, \n Title = {Multi-Attribute Bias Mitigation via \n  Representation  Learning}, \n Booktitle = {ECAI}, \n Year   = {2025}}\n}"
    }    
  },
  
  {
    "title": "Label Calibration in Source Free Domain Adaptation",
    "image": "/images/publications/eks.png",
    "authors": "Shivangi Rai, Rini Smita Thakur, Kunal Jangid, Vinod K Kurmi",
    "conference": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2025",
    "links":{
      "page": "https://visdomlab.github.io/EKS/",
      "pdf":"https://openaccess.thecvf.com/content/WACV2025/papers/Rai_Label_Calibration_in_Source_Free_Domain_Adaptation_WACV_2025_paper.pdf"
    },
    "details":{
      "abstract":"Source-free domain adaptation (SFDA) utilizes a pre-trained source model with unlabeled target data. Self-supervised SFDA techniques generate pseudolabels from the pre-trained source model, but these pseudolabels often contain noise due to domain discrepancies between the source and target domains. Traditional self-supervised SFDA techniques rely on deterministic model predictions using the softmax function, leading to unreliable pseudolabels. In this work, we propose to introduce predictive uncertainty and softmax calibration for pseudolabel refinement using evidential deep learning. The Dirichlet prior is placed over the output of the target network to capture uncertainty using evidence with a single forward pass. Furthermore, softmax calibration solves the translation invariance problem to assist in learning with noisy labels. We incorporate a combination of evidential deep learning loss and information maximization loss with calibrated softmax in both prior and non-prior target knowledge SFDA settings. Extensive experimental analysis shows that our method outperforms other state-of-the-art methods on benchmark datasets. ",
      "bibtex":"@InProceedings{eks_es_WACV2025,\nauthor    = {Rai, Shivangi, \nand Thakur, Rini Smita, \nJangid, Kunal \nand Kurmi, Vinod},\ntitle     = {Evidential Deep Learning for \nPseudolabel Refinement in Source Free \nDomain Adaptation},\nbooktitle = {Proceedings of the IEEE/CVF Winter \nConference on Applications of Computer Vision \n(WACV)},\nyear      = {2025},\n}"
    }    
  },
  {
    "title": "Uncertainty and Energy based Loss Guided Semi-Supervised Semantic Segmentation",
    "image": "/images/publications/DUEB.png",
    "authors": "Rini Smita Thakur, Vinod K Kurmi",
    "conference": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2025",
    "links":{
      "page" :"https://visdomlab.github.io/DUEB/",
      "pdf":"https://openaccess.thecvf.com/content/WACV2025/papers/Thakur_Uncertainty_and_Energy_Based_Loss_Guided_Semi-Supervised_Semantic_Segmentation_WACV_2025_paper.pdf"
    },
    "details":{
      "abstract":"Semi-supervised (SS) semantic segmentation exploits both labeled and unlabeled images to overcome tedious and costly pixel-level annotation problems. Pseudolabel supervision is one of the core approaches of training networks with both pseudo labels and ground-truth labels. This work uses aleatoric or data uncertainty and energy based modeling in intersection-union pseudo supervised network.The aleatoric uncertainty is modeling the inherent noise variations of the data in a network with two predictive branches. The per-pixel variance parameter obtained from the network gives a quantitative idea about the data uncertainty. Moreover, energy-based loss realizes the potential of generative modeling on the downstream SS segmentation task. The aleatoric and energy loss are applied in conjunction with pseudo-intersection labels, pseudo-union labels, and ground-truth on the respective network branch. The comparative analysis with state-of-the-art methods has shown improvement in performance metrics.",
      "bibtex":"@InProceedings{Thakur_2025_WACV,\n Author = {Thakur, Rini Smita \nand Kurmi, Vinod K. }, \nTitle = {Uncertainty and Energy based Loss \nGuided Semi-Supervised Semantic Segmentation},\nBooktitle = {Proceedings of the IEEE/CVF Winter \nConference on Applications of Computer Vision \n(WACV)}, \nmonth = {Feb}, Year = {2025}\n}"
    }    
  },
  {
    "title": "Strategic Base Representation Learning via Feature Augmentations for Few-Shot Class Incremental Learning",
    "image": "/images/publications/FeatAugFSCIL.png",
    "authors": "Parinita Nema, Vinod K Kurmi",
    "conference": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2025",
    "links":{
      "page" :"https://visdomlab.github.io/FeatAugFSCIL/",
      "pdf":"https://openaccess.thecvf.com/content/WACV2025/papers/Nema_Strategic_Base_Representation_Learning_via_Feature_Augmentations_for_Few-Shot_Class_WACV_2025_paper.pdf"
    },
    "details":{
      "abstract":" Few-shot class incremental learning implies the model to learn new classes while retaining knowledge of previously learned classes with a small number of training instances. Existing frameworks typically freeze the parameters of the previously learned classes during the incorporation of new classes. However, this approach often results in suboptimal class separation of previously learned classes,  leading to overlap between old and new classes. Consequently, the performance of old classes degrades on new classes. To address these challenges, we propose a novel feature augmentation driven contrastive learning framework designed to enhance the separation of previously learned classes to accommodate new classes. Our approach involves augmenting feature vectors and assigning proxy labels to these vectors. This strategy expands the feature space, ensuring seamless integration of new classes within the expanded space. Additionally, we employ a self-supervised contrastive loss to improve the separation between previous classes. We validate our framework through experiments on three FSCIL benchmark datasets: CIFAR100, miniImageNet, and CUB200. The results demonstrate that our Feature Augmentation driven Contrastive Learning framework significantly outperforms other approaches, achieving state-of-the-art performance.",
      "bibtex":"@inproceedings{nema_wacv2025,\ntitle={Strategic Base Representation Learning \nvia Feature Augmentations for Few-Shot Class \nIncremental Learning},\nauthor={Nema, Parinita \nand Kurmi, Vinod K},\nbooktitle={Proc. IEEE/CVF Winter Conference on \nApplications of Computer Vision (WACV)},\nyear={2025}\n}"
    }    
  },
  {
    "title": "CosFairNet: A Parameter-Space based Approach for Bias Free Learning",
    "image": "/images/publications/CosFairNet.png",
    "authors": "Rajeev Ranjan Dwivedi, Priyadarshni Kumari, Vinod K Kurmi",
    "conference": "British Machine Vision Conference (BMVC), 2024",
    "links":{
      "page": "https://visdomlab.github.io/CosFairNet/",
      "pdf":"https://bmva-archive.org.uk/bmvc/2024/papers/Paper_738/paper.pdf"
    },
    "details":{
      "abstract":"Deep neural networks trained on biased data often inadvertently learn unintended inference rules, particularly when labels are strongly correlated with biased features. Existing bias mitigation methods typically involve either a) predefining bias types and enforcing them as prior knowledge or b) reweighting training samples to emphasize bias-conflicting samples over bias-aligned samples. However, both strategies address bias indirectly in the feature or sample space, with no control over learned weights, making it difficult to control the bias propagation across different layers. Based on this observation, we introduce a novel approach to address bias directly in the model's parameter space, preventing its propagation across layers. Our method involves training two models: a bias model for biased features and a debias model for unbiased details, guided by the bias model. We enforce dissimilarity in the debias model's later layers and similarity in its initial layers with the bias model, ensuring it learns unbiased low-level features without adopting biased high-level abstractions. By incorporating this explicit constraint during training, our approach shows enhanced classification accuracy and debiasing effectiveness across various synthetic and real-world datasets of different sizes. Moreover, the proposed method demonstrates robustness across different bias types and percentages of biased samples in the training data.",
      "bibtex":"@InProceedings{rajeevbmvc24_abs,\nauthor    = {Dwivedi, Rajeev R \nand Kumari, Priyadarshini \nand Kurmi, Vinod},\ntitle     = {CosFairNet:A Parameter-Space based \nApproach for Bias Free Learning},\nbooktitle = {Proceedings of the British Machine \nVision Conference (BMVC)},\nyear      = {2024},\n}"
    }    
  },
  {
    "title": "Quantifying Uncertainty in Neural Networks through Residuals",
    "image": "/images/publications/Quantifying.png",
    "authors": "Udhbav Dalavi, Rini Smita Thakur, Rajeev Ranjan Dwivedi, Vinod K Kurmi",
    "conference": "33rd ACM International Conference on Information and Knowledge Management (CIKM), 2024",
    "links":{
      "page" :"https://visdomlab.github.io/HetGP/",
      "pdf":"https://dl.acm.org/doi/pdf/10.1145/3627673.3679983"
    },
    "details":{
      "abstract":"Regression models are of fundamental importance in explicitly explaining the response variable in terms of covariates. Heteroscedasticity is common in most real-world scenarios and is hard to model due to its randomness. The Gaussian process generally captures epistemic (model) uncertainty but fails to capture heteroscedastic aleatoric uncertainty. The framework of Heteroscedastic Gaussian Process (HetGP) inherently captures both epistemic and aleatoric by placing independent Gaussian processes on both mean function and error term. In this work, we propose the posthoc HetGP on the residuals of the trained neural network to obtain both epistemic and aleatoric uncertainty. The advantage of posthoc HetGP on residuals is that it can be extended to any type of model, since the model is assumed to be black-box that gives point predictions. We demonstrate our approach through simulation studies on UCI regression datasets.",
      "bibtex":"@inproceedings{udbhav,\n Author = {Udbhav Mallanna,Dalavai \nand Dwivedi, Rajeev R \nand Thakur, Rini S \nand Kurmi, Vinod }, \nTitle = {Quantifying Uncertainty \nin Neural Networks through Residuals},\nBooktitle = {CIKM},\nYear   = {2024}\n}"
    }    
  },
  {
    "title": "Towards Robust Few-shot Class Incremental Learning in Audio Classification using Contrastive Representation",
    "image": "/images/publications/audio_classification.png",
    "authors": "Riyansha Singh, Parinita Nema, Vinod K Kurmi",
    "conference": "Interspeech 2024",
    "links":{
      "page" :"https://visdomlab.github.io/FsACLearning/",
      "pdf":"https://arxiv.org/abs/2407.19265"
    },
    "details":{
      "abstract":"In machine learning applications, gradual data ingress is common, especially in audio processing where incremental learning is vital for real-time analytics. Few-shot class-incremental learning addresses challenges arising from limited incoming data. Existing methods often integrate additional trainable components or rely on a fixed embedding extractor post-training on base sessions to mitigate concerns related to catastrophic forgetting and the dangers of model overfitting. However, using cross-entropy loss alone during base session training is suboptimal for audio data. To address this, we propose incorporating supervised contrastive learning to refine the representation space, enhancing discriminative power and leading to better generalization since it facilitates seamless integration of incremental classes, upon arrival. Experimental results on NSynth and LibriSpeech datasets with 100 classes, as well as ESC dataset with 50 and 10 classes, demonstrate state-of-the-art performance.",
      "bibtex":"@article{singh2024towards,\ntitle={Towards Robust Few-shot Class \nIncremental Learning in Audio Classification \nusing Contrastive Representation},\nauthor={Singh, Riyansha \nand Nema, Parinita \nand Kurmi, Vinod K},\njournal={arXiv preprint arXiv:2407.19265},\nyear={2024}\n}"
    }    
  },
  {
    "title": "Distilling Knowledge for Occlusion Robust Monocular 3D Face Reconstruction",
    "image": "/images/publications/ivc_3.jpg",
    "authors": "H. Tiwari, Vinod K Kurmi, Venkatesh K Subramanian, Yong-Sheng Chen",
    "conference": "Image and Vision Computing,(IMAVIS), 2023",
    "links":{
      "pdf":"https://sciencedirect.com/science/article/abs/pii/S0262885623001373"
    },
    "details":{
      "abstract":"Recently, there have been significant advancements in the 3D face reconstruction field, largely driven by monocular image-based deep learning methods. However, these methods still face challenges in reliable deployments due to their sensitivity to facial occlusions and inability to maintain identity consistency across different occlusions within the same facial image. To address these issues, we propose two frameworks: Distillation Assisted Mono Image Occlusion Robustification (DAMIOR) and Duplicate Images Assisted Multi Occlusions Robustification (DIAMOR). The DAMIOR framework leverages the knowledge from the Occlusion Frail Trainer (OFT) network to enhance robustness against facial occlusions. Our proposed method overcomes the sensitivity to occlusions and improves reconstruction accuracy. To tackle the issue of identity inconsistency, the DIAMOR framework utilizes the estimates from DAMIOR to mitigate inconsistencies in geometry and texture, collectively known as identity, of the reconstructed 3D faces. We evaluate the performance of DAMIOR on two variations of the CelebA test dataset: empirical occlusions and irrational occlusions. Furthermore, we analyze the performance of the proposed DIAMOR framework using the irrational occlusion-based variant of the CelebA test dataset. Our methods outperform state-of-the-art approaches by a significant margin. For example, DAMIOR reduces the 3D vertex-based shape error by 41.1% and the texture error by 21.8% for empirical occlusions. Besides, for facial data with irrational occlusions, DIAMOR achieves a substantial decrease in shape error by 42.5% and texture error by 30.5%. These results demonstrate the effectiveness of our proposed methods.",
      "bibtex":"@inproceedings{hitika_ivc3,\nAuthor = {Tiwari, Hitika\nand Kurmi, Vinod K\nand  Subramanian,\nVenkatesh K and\nChen, Yong Sheng },\nTitle = {Distilling Knowledge\nfor Occlusion Robust Monocular\n3D Face Reconstruction},\nBooktitle = {InterSpeech},\nYear = {2022}\n}"
    }    
  },
  {
    "title": "Generalized Keyword Spotting using ASR embeddings",
    "image": "/images/publications/intersp.jpg",
    "authors": "Kirandevraj R, Vinod K Kurmi, Vinay P Namboodiri, C V Jawahar",
    "conference": "Conference of the International Speech Communication Association (Interspeech) 2022, Incheon Korea",
    "links":{
      "pdf":"https://www.isca-speech.org/archive/pdfs/interspeech_2022/r22_interspeech.pdf"
    },
    "details":{
      "abstract":"Keyword Spotting (KWS) detects a set of pre-defined spoken keywords. Building a KWS system for an arbitrary set re- quires massive training datasets. We propose to use the text transcripts from an Automatic Speech Recognition (ASR) sys- tem alongside triplets for KWS training. The intermediate rep- resentation from the ASR system trained on a speech corpus is used as acoustic word embeddings for keywords. Triplet loss is added to the Connectionist Temporal Classification (CTC) loss in the ASR while training. This method achieves an Average Precision (AP) of 0.843 over 344 words unseen by the model trained on the TIMIT dataset. In contrast, the Multi-View re- current method that learns jointly on the text and acoustic em- beddings achieves only 0.218 for out-of-vocabulary words. This method is also applied to low-resource languages such as Tamil by converting Tamil characters to English using transliteration. This is a very challenging novel task for which we provide a dataset of transcripts for the keywords. Despite our model not generalizing well, we achieve a benchmark AP of 0.321 on over 38 words unseen by the model on the MSWC Tamil keyword set. The model also produces an accuracy of 96.2% for classifi- cation tasks on the Google Speech Commands dataset.",
      "bibtex":"@inproceedings{kiran_inter22,\nAuthor = {R,Kiran.\nand Kurmi, Vinod K\nand Namboodiri, Vinay P\nand Jawhar, CV},\nTitle = {Generalized Keyword\nSpotting using ASR embeddings},\nBooktitle = {InterSpeech},\nYear   = {2022}\n}\n}"
    }    
  }
]